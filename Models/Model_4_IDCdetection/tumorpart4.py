# -*- coding: utf-8 -*-
"""Tumorpart4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AZxLtl4BTGQCzIG5PCfs-R6BX_myNAH0
"""

#%tensorflow_version 2.x
#TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping

import numpy as np
import matplotlib.pyplot as plt
import os

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d paultimothymooney/breast-histopathology-images
!unzip -q breast-histopathology-images.zip -d IDC_images

import os
import glob
import cv2
import numpy as np
from PIL import Image
from sklearn.model_selection import train_test_split

# Step 1: Set path to extracted dataset
data_dir = '/content/IDC_images'  # This is where you unzipped the dataset
image_size = 50

X = []
y = []

# Step 2: Load all images and labels
for patient_folder in os.listdir(data_dir):
    for label in ['0', '1']:
        folder_path = os.path.join(data_dir, patient_folder, label)
        if not os.path.exists(folder_path):
            continue
        for img_file in glob.glob(os.path.join(folder_path, '*.png')):
            img = cv2.imread(img_file)
            img = cv2.resize(img, (image_size, image_size))
            X.append(img)
            y.append(int(label))  # Don't use one-hot here

X = np.array(X)
y = np.array(y)

# Step 3: Split into train/val/test
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

# Step 4: Save images to directory format
base_dir = "/content/IDC_images_split"
for split in ['train', 'val', 'test']:
    for cls in ['0', '1']:
        os.makedirs(os.path.join(base_dir, split, cls), exist_ok=True)

def save_images(X_data, y_data, split_name):
    for i in range(len(X_data)):
        label = str(y_data[i])  # Already 0 or 1
        img = Image.fromarray(X_data[i].astype(np.uint8))  # CV2 loads as uint8
        path = os.path.join(base_dir, split_name, label, f"{label}_{i}.png")
        img.save(path)

save_images(X_train, y_train, 'train')
save_images(X_val, y_val, 'val')
save_images(X_test, y_test, 'test')

print(f"Total images loaded: {len(X)}")
print(f"Shape of first image: {X[0].shape}")
print(f"Sample labels: {y[:10]}")

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Paths to directories
train_path = '/content/IDC_images_split/train'
val_path = '/content/IDC_images_split/val'
test_path = '/content/IDC_images_split/test'

# ImageDataGenerators
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
)

val_test_datagen = ImageDataGenerator(rescale=1./255)

# Flow from directories
train_generator = train_datagen.flow_from_directory(
    train_path,
    target_size=(50, 50),
    batch_size=64,
    class_mode='binary'
)

validation_generator = val_test_datagen.flow_from_directory(
    val_path,
    target_size=(50, 50),
    batch_size=64,
    class_mode='binary',
    shuffle=False
)

test_generator = val_test_datagen.flow_from_directory(
    test_path,
    target_size=(50, 50),
    batch_size=64,
    class_mode='binary',
    shuffle=False
)

from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping

# Define CNN model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(50, 50, 3)),
    layers.MaxPooling2D((2, 2)),
    # layers.Dropout(0.1),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    # layers.Dropout(0.1),

    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    # layers.Dropout(0.1),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    # layers.Dropout(0.2),
    layers.Dense(1, activation='sigmoid')  # Binary output
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Print model summary
model.summary()

# Set early stopping
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# #Limit number of samples per class for speed (e.g., 10000/class for training)
# train_generator = train_datagen.flow_from_directory(
#     train_path,
#     target_size=(50, 50),
#     batch_size=64,
#     class_mode='binary',
#     shuffle=True,
#     subset='training'
# )

# validation_generator = val_test_datagen.flow_from_directory(
#     val_path,
#     target_size=(50, 50),
#     batch_size=64,
#     class_mode='binary',
#     shuffle=False
# )

# test_generator = val_test_datagen.flow_from_directory(
#     test_path,
#     target_size=(50, 50),
#     batch_size=64,
#     class_mode='binary',
#     shuffle=False
# )
# #limiting end

# Train the model
history = model.fit(
    train_generator,
    epochs=10,
    validation_data=validation_generator,
    callbacks=[early_stopping]
)

test_loss, test_acc = model.evaluate(test_generator)
print("âœ… Test Accuracy:", test_acc)