{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Path to the folder\n",
        "nifti_path = \"/content/drive/MyDrive/Glioma_NIfTI_Dataset/UCSF-PDGM-v1.0-nifti\"\n",
        "csv_path = \"/content/drive/MyDrive/Glioma_NIfTI_Dataset/UCSF-PDGM-metadata_v5.csv\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogwu78iFCOks",
        "outputId": "d6d26c77-fc9a-484b-9af7-861e460cf4f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nibabel\n",
        "!pip install opencv-python\n",
        "!pip install nilearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdB_I5QGr-rA",
        "outputId": "3a61d3ce-972c-4e1e-ad26-cc9d269163f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (5.3.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel) (6.5.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from nibabel) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.11/dist-packages (from nibabel) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel) (4.14.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: nilearn in /usr/local/lib/python3.11/dist-packages (0.12.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (1.5.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from nilearn) (5.4.0)\n",
            "Requirement already satisfied: nibabel>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (5.3.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from nilearn) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nilearn) (25.0)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from nilearn) (1.15.3)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel>=5.2.0->nilearn) (6.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from nibabel>=5.2.0->nilearn) (4.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->nilearn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->nilearn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.0->nilearn) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->nilearn) (2025.7.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.0->nilearn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->nilearn) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Filter rows with WHO CNS Grade 2, 3, or 4\n",
        "df = df[df['WHO CNS Grade'].isin(['2', '3', '4'])]\n",
        "\n",
        "# Keep only subject ID and grade, and clean column names\n",
        "df = df[['ID', 'WHO CNS Grade']]\n",
        "df['WHO CNS Grade'] = df['WHO CNS Grade'].astype(int)\n",
        "df = df.rename(columns={'WHO CNS Grade': 'Grade'})"
      ],
      "metadata": {
        "id": "FVLhl9z2z5pQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "print(\"Column names in CSV:\")\n",
        "print(list(df.columns))\n"
      ],
      "metadata": {
        "id": "G-84F5DO4S5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "nifti_root = '/content/drive/MyDrive/Glioma_NIfTI_Dataset/'  # or wherever your dataset is\n",
        "sample_subjects = os.listdir(nifti_root)\n",
        "print(\"Example subject folders/files:\", sample_subjects[:5])\n",
        "\n",
        "for subject_id in df['ID'].values[:5]:\n",
        "    subject_path = os.path.join(nifti_root, subject_id)\n",
        "    print(f\"Checking: {subject_path} -> Exists: {os.path.exists(subject_path)}\")\n"
      ],
      "metadata": {
        "id": "iPHNVSpo43YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_full = pd.read_csv(csv_path)\n",
        "print(\"Unique WHO Grades:\", df_full['WHO CNS Grade'].unique())\n"
      ],
      "metadata": {
        "id": "hrbbb-Vd4VVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#1. Paths\n",
        "nifti_root = \"/content/drive/MyDrive/Glioma_NIfTI_Dataset/PKG - UCSF-PDGM_NIfTI_for_BraTS2021/UCSF-PDGM-for-BraTS2021/UCSF-PDGM-nifti/\"\n",
        "csv_path = \"/content/drive/MyDrive/Glioma_NIfTI_Dataset/UCSF-PDGM-metadata_v5.csv\"\n",
        "\n",
        "#2. Load metadata\n",
        "df = pd.read_csv(csv_path)\n",
        "print(f\"✅ Metadata loaded. Shape: {df.shape}\")\n",
        "\n",
        "#3. Scan all FLAIR files\n",
        "flair_map = {}\n",
        "for root, dirs, files in os.walk(nifti_root):\n",
        "    for file in files:\n",
        "        if file.endswith(\"_FLAIR.nii.gz\"):\n",
        "            # Extract ID like 'UCSF-PDGM-0529' from filename\n",
        "            subject_id = file.split(\"_FLAIR\")[0]\n",
        "            flair_map[subject_id] = os.path.join(root, file)\n",
        "\n",
        "print(f\"🧠 Found {len(flair_map)} FLAIR files.\")\n",
        "\n",
        "import re\n",
        "\n",
        "def normalize_id(subject_id):\n",
        "    match = re.search(r'UCSF-PDGM-(\\d+)', subject_id)\n",
        "    if match:\n",
        "        return f\"UCSF-PDGM-{int(match.group(1)):04d}\"\n",
        "    else:\n",
        "        return None  # or subject_id if you want to keep unmatched ones\n",
        "\n",
        "df[\"ID\"] = df[\"ID\"].apply(normalize_id)\n",
        "df = df.dropna(subset=[\"ID\"]) #Drops any rows where ID normalization failed\n",
        "\n",
        "#4. Load images and labels\n",
        "X = []\n",
        "y = []\n",
        "missing_ids = []\n",
        "\n",
        "def load_nifti_image(path):\n",
        "    img = nib.load(path).get_fdata()\n",
        "    # Take central slice from 3D volume (axis 2), normalize, resize\n",
        "    img_slice = img[:, :, img.shape[2] // 2]\n",
        "    img_slice = cv2.resize(img_slice, (128, 128))\n",
        "    img_slice = img_slice / np.max(img_slice)\n",
        "    return img_slice\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    subject_id = row[\"ID\"]\n",
        "    grade = row[\"WHO CNS Grade\"]\n",
        "\n",
        "    if pd.isna(grade) or subject_id not in flair_map:\n",
        "        missing_ids.append(subject_id)\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        path = flair_map[subject_id]\n",
        "        img_array = load_nifti_image(path)\n",
        "        X.append(img_array)\n",
        "        y.append(int(grade))\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error loading {subject_id}: {e}\")\n",
        "        missing_ids.append(subject_id)\n",
        "\n",
        "print(f\"❌ Missing FLAIR for {len(missing_ids)} subjects: {missing_ids[:5]}\")\n",
        "print(f\"✅ Loaded {len(X)} samples.\")\n",
        "\n",
        "# 5. Preprocess\n",
        "if len(X) == 0:\n",
        "    raise ValueError(\"❌ No images were loaded. Check paths or file structure.\")\n",
        "\n",
        "X = np.array(X).reshape(-1, 128, 128, 1)\n",
        "y = to_categorical(np.array(y) - 2, num_classes=3)  # Grades 2/3/4 → 0/1/2\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"✅ Data split: Train {len(X_train)}, Test {len(X_test)}\")\n"
      ],
      "metadata": {
        "id": "I2VifwaUJ8uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# ⚠️ Ensure y is properly shaped and encoded\n",
        "y = np.array(y)\n",
        "if len(y.shape) > 1:\n",
        "    y = y.argmax(axis=-1)  # undo previous one-hot if already done\n",
        "\n",
        "# 🧠 Encode WHO Grades (2, 3, 4) → (0, 1, 2)\n",
        "if len(y.shape) == 1:\n",
        "    y = y - 2  # encode labels 2,3,4 -> 0,1,2\n",
        "    y = to_categorical(y, num_classes=3)\n",
        "\n",
        "\n",
        "# 📦 Reshape image data\n",
        "X = np.array(X).reshape(-1, 128, 128, 1)\n",
        "\n",
        "# 📊 Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 🏗️ Define CNN model\n",
        "model = Sequential([\n",
        "    Input(shape=(128, 128, 1)),  # 👈 Input layer\n",
        "    Conv2D(32, kernel_size=3, activation='relu'),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Conv2D(64, kernel_size=3, activation='relu'),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation='softmax')  # 3 output classes (Grade 2, 3, 4)\n",
        "])\n",
        "\n",
        "# ⚙️ Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 🧪 Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "tkn4iYFl6xi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "\n",
        "# 🧹 Step 1: Ensure X is numeric and already 4D (with channel)\n",
        "X = np.array(X, dtype=np.float32)\n",
        "if X.ndim != 4 or X.shape[-1] != 1:\n",
        "    raise ValueError(f\"Expected X to have shape (n, 128, 128, 1), but got {X.shape}\")\n",
        "\n",
        "# 🎯 Step 2: Process labels\n",
        "y = np.array(y)\n",
        "print(\"Unique labels in y:\", np.unique(y))\n",
        "\n",
        "if np.any((y < 2) | (y > 4)):\n",
        "    raise ValueError(\"Labels must be WHO Grades 2, 3, or 4 only.\")\n",
        "\n",
        "# Encode WHO Grades: (2, 3, 4) → (0, 1, 2)\n",
        "y_encoded = y - 2\n",
        "\n",
        "# ✅ Step 3: Train/test split\n",
        "X_train, X_test, y_train_raw, y_test_raw = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# 🔢 Step 4: One-hot encode labels\n",
        "y_train = to_categorical(y_train_raw, num_classes=3)\n",
        "y_test = to_categorical(y_test_raw, num_classes=3)\n",
        "\n",
        "# 🏗️ Step 5: Define CNN model\n",
        "model = Sequential([\n",
        "    Input(shape=(128, 128, 1)),\n",
        "    Conv2D(32, kernel_size=3, activation='relu'),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Conv2D(64, kernel_size=3, activation='relu'),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation='softmax')  # Output layer for 3 classes\n",
        "])\n",
        "\n",
        "# ⚙️ Step 6: Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 🚀 Step 7: Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=30,\n",
        "    batch_size=16,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 🧪 Step 8: Evaluate model on test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\n✅ Final Test Accuracy: {test_accuracy:.4f} | Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "KzcC0bLHT4mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ✅ Path where your .nii.gz files are stored\n",
        "data_dir = \"/content/drive/MyDrive/Glioma_NIfTI_Dataset/\"\n",
        "\n",
        "# Collect paths and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "for file in os.listdir(data_dir):\n",
        "    if file.endswith(\"_FLAIR.nii.gz\"):\n",
        "        patient_id = file.split(\"_\")[0]  # e.g., UCSF-PDGM-0529\n",
        "        label = id_to_grade.get(patient_id)\n",
        "        if label in [2, 3, 4]:\n",
        "            image_paths.append(os.path.join(data_dir, file))\n",
        "            labels.append(label)\n",
        "\n",
        "print(f\"Total images found: {len(image_paths)}\")\n",
        "print(f\"Sample label distribution: {np.unique(labels, return_counts=True)}\")\n",
        "\n",
        "import nibabel as nib\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Convert WHO Grades to 0, 1, 2 for classification\n",
        "labels = [l - 2 for l in labels]  # 2→0, 3→1, 4→2\n",
        "\n",
        "# Load MRI slices (center slice of FLAIR for simplicity)\n",
        "def load_image(file_path):\n",
        "    img = nib.load(file_path).get_fdata()\n",
        "    center_slice = img.shape[2] // 2\n",
        "    slice_2d = img[:, :, center_slice]\n",
        "    resized = cv2.resize(slice_2d, (128, 128))\n",
        "    return resized.astype(np.float32) / 255.0\n",
        "\n",
        "X = np.array([load_image(p) for p in image_paths])\n",
        "X = np.expand_dims(X, axis=-1)  # shape → (N, 128, 128, 1)\n",
        "y = np.array(labels)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "print(\"Total number of images:\", len(X))\n",
        "print(\"Shape of X:\", X.shape)\n",
        "print(\"Shape of y:\", y.shape)\n",
        "print(\"Sample labels:\", np.unique(y, return_counts=True))\n",
        "\n",
        "\n",
        "# 📊 Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train_cat = to_categorical(y_train, num_classes=3)\n",
        "y_test_cat = to_categorical(y_test, num_classes=3)\n"
      ],
      "metadata": {
        "id": "kHZ3vNV3W7Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "IMG_SIZE = 150\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "classes = [\"Grade_2\", \"Grade_3\", \"Grade_4\"]\n",
        "\n",
        "for label, grade in enumerate(classes):\n",
        "    folder_path = os.path.join(nifti_path, grade)\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.endswith(\".nii\") or file.endswith(\".nii.gz\"):\n",
        "            img_path = os.path.join(folder_path, file)\n",
        "            nii = nib.load(img_path)\n",
        "            img_data = nii.get_fdata()\n",
        "\n",
        "            mid_slice = img_data.shape[2] // 2\n",
        "            img_slice = img_data[:, :, mid_slice]\n",
        "\n",
        "            img_slice = cv2.resize(img_slice, (IMG_SIZE, IMG_SIZE))\n",
        "            img_slice = img_slice / np.max(img_slice)  # Normalize\n",
        "\n",
        "            X.append(img_slice)\n",
        "            y.append(label)\n",
        "\n",
        "X = np.array(X)\n",
        "X = np.expand_dims(X, axis=-1)  # Add channel dimension\n",
        "y = np.array(y)\n",
        "\n",
        "# STEP 4: Train-Test Split and One-Hot Encoding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=3)\n",
        "y_test = to_categorical(y_test, num_classes=3)\n",
        "\n",
        "# STEP 5: Define the CNN Model\n",
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# STEP 6: Train the Model\n",
        "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
        "\n",
        "# STEP 7: Evaluate the Model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test Accuracy:\", test_acc)"
      ],
      "metadata": {
        "id": "ZfRjfP3zsEtt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}